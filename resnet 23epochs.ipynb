{"cells":[{"metadata":{"id":"8BTa6SiPqTUP"},"cell_type":"markdown","source":"## Necessary preparations"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"cEu_ucG7oqCj","outputId":"138d6f74-81e5-4abe-b5bc-b51ab122f267"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\n\nimport PIL\nprint(PIL.PILLOW_VERSION)\n\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"-iKibSNIoqCp"},"cell_type":"code","source":"import pickle\nimport numpy as np\nfrom skimage import io\nimport random\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom PIL import Image\nfrom pathlib import Path\n\nfrom torchvision import transforms\nfrom multiprocessing.pool import ThreadPool\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\nfrom matplotlib import colors, pyplot as plt\n%matplotlib inline\n\n# в sklearn не все гладко, чтобы в colab удобно выводить картинки \n# мы будем игнорировать warnings\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"h8Y7kcyboqCt"},"cell_type":"code","source":"SEED = 42\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"vHLdbpeyoqCw"},"cell_type":"code","source":"DATA_MODES = ['train', 'val', 'test']\nRESCALE_SIZE = 224\nDEVICE = torch.device(\"cuda\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(torch.__version__)","execution_count":null,"outputs":[]},{"metadata":{"id":"HR8eve9DoqC0"},"cell_type":"markdown","source":"# Dataset construction"},{"metadata":{"trusted":true,"id":"HGHo9qJpoqC1"},"cell_type":"code","source":"class SimpsonsDataset(Dataset):\n  def __init__(self, files, mode, augmentations = None):\n    super().__init__()\n    self.files = files\n    self.mode = mode\n    self.augmentations = augmentations\n\n    if self.mode not in DATA_MODES:\n      print(f'wrong mode: {self.mode}')\n      raise NameError\n\n    self.len_ = len(self.files)\n    self.label_encoder = LabelEncoder()\n\n    if self.mode != 'test':\n      self.labels = [path.parent.name for path in self.files]\n      self.label_encoder.fit(self.labels)\n\n      with open('label_encoder.pkl', 'wb') as le_dump:\n        pickle.dump(self.label_encoder, le_dump)\n\n  def __len__(self):\n    return self.len_\n\n  def load_sample(self, file):\n    image = Image.open(file)\n    image.load()\n    return image\n\n  def __getitem__(self, index):\n    transform = transforms.Compose([\n      transforms.ToTensor(),\n      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                \n    ])\n    # трансформации для шума\n    custom_augmentations_01 = transforms.RandomOrder([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomApply([transforms.RandomRotation(degrees=10)], p=0.25),\n    transforms.RandomApply([transforms.RandomResizedCrop(224, scale=(0.8, 1.25), ratio=(0.8, 1.25))], p=0.25),\n    transforms.RandomApply([transforms.RandomAffine((-10,10), (0.1,0.1))], p=0.25),\n    transforms.RandomPerspective(distortion_scale=0.1, p=0.25),\n    transforms.RandomApply([transforms.ColorJitter(brightness=0.02,contrast=0.02,saturation=0.02,hue=(-0.02, 0.02))], p=0.01)\n    ])\n\n    x = self.load_sample(self.files[index])\n    x = self._prepare_sample(x)\n    #x = np.array(x / 255, dtype='float32')\n\n    \n    if self.mode == 'test':\n        x = np.array(x)\n        x = np.array(x / 255, dtype='float32')\n        x = transform(x)\n        return x\n    else:\n        \n        if self.mode == 'train':\n            x = custom_augmentations_01(x)\n            \n        x = transform(x)\n        \n        label = self.labels[index]\n        label_id = self.label_encoder.transform([label])\n        y = label_id.item()\n        return x, y\n\n  def _prepare_sample(self, image):\n    image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n    return image #np.array(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"v6nXCN8OoqC4"},"cell_type":"code","source":"TRAIN_DIR = Path('/kaggle/input/simpsons4/train')\nTEST_DIR = Path('/kaggle/input/simpsons4/testset/testset')\n\ntrain_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\ntest_files = sorted(list(TEST_DIR.rglob('*.jpg')))","execution_count":null,"outputs":[]},{"metadata":{"id":"OcxD5mZ5oqC7"},"cell_type":"markdown","source":"Тренируем модель на всей выборке, чтобы точно задействовать все классы. При этом валидационная выборка - это 30 процентов от тех же данных, НО аугментируем мы только тренировочную часть."},{"metadata":{"trusted":true,"id":"qlnWTgaLoqC8"},"cell_type":"code","source":"from sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\ntrain_val_labels = [path.parent.name for path in train_val_files]\ntrain_files, val_files = train_test_split(train_val_files, test_size=0.3, \\\n                                          stratify=train_val_labels)\n\ntrain_files = shuffle(train_val_files, random_state=0) #!!!\n\nval_dataset = SimpsonsDataset(val_files, mode='val')\ntrain_dataset = SimpsonsDataset(train_files, mode='train')","execution_count":null,"outputs":[]},{"metadata":{"id":"eeSpgW0MoqC_"},"cell_type":"markdown","source":"# Let's take a look at our data"},{"metadata":{"trusted":true,"id":"nTloniALoqDA"},"cell_type":"code","source":"    \ndef imshow(img, title=None, plt_ax=plt, default=False):\n  img = img.numpy().transpose((1, 2, 0))\n  mean = np.array([0.485, 0.456, 0.406])\n  std = np.array([0.229, 0.224, 0.225])\n  img = std * img + mean\n  img = np.clip(img, 0, 1)\n  plt_ax.imshow(img)\n  if title is not None:\n    plt_ax.set_title(title)\n  plt_ax.grid(False)\n\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(10,10), sharex=True, sharey=True)\n\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0,1000))\n    im_val, label = val_dataset[random_characters]\n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n    imshow(im_val.data.cpu(), \\\n          title=img_label,plt_ax=fig_x)\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"GLQ5mBe4oqDE"},"cell_type":"markdown","source":"# Building the model"},{"metadata":{"id":"d3QxAoE0pc-2"},"cell_type":"markdown","source":"Импортируем готовую resnet34 и заменим последний слой."},{"metadata":{"trusted":true,"id":"_b3Jx--zoqDI"},"cell_type":"code","source":"from torchvision import models\n\nmodel = models.resnet34(pretrained=True)\n#model = models.resnet18(pretrained=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"OmyIvGGSoqDL"},"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = True\n    \nmodel.fc = nn.Linear(in_features=model.fc.in_features, out_features=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"VaODCZkxsvxy","trusted":true},"cell_type":"code","source":"model = model.to(DEVICE)","execution_count":null,"outputs":[]},{"metadata":{"id":"yRmihlwlsN83"},"cell_type":"markdown","source":"Создадим циклический lr_scheduler:"},{"metadata":{"id":"ve-3027csU0N","trusted":true},"cell_type":"code","source":"import math\n\ndef cyclical_lr(stepsize, min_lr=1e-3, max_lr=1e-2):\n    \n    # Scaler: we can adapt this if we do not want the triangular CLR\n    scaler = lambda x: 1.\n    \n    # Additional function to see where on the cycle we are\n    def relative(it, stepsize):\n        cycle = math.floor(1 + it / (2 * stepsize))\n        x = abs(it / stepsize - 2 * cycle + 1)\n        return max(0, (1 - x)) * scaler(cycle)\n\n    # Lambda function to calculate the LR\n    lr_lambda = lambda it: min_lr + (max_lr - min_lr) * relative(it, stepsize)\n    print('cycle')\n    return lr_lambda","execution_count":null,"outputs":[]},{"metadata":{"id":"DzVsz5NpoqDO"},"cell_type":"markdown","source":"Функции для обучения, проверки, предсказаний."},{"metadata":{"trusted":true,"id":"HrQRZjVQoqDP"},"cell_type":"code","source":"def fit_epoch(model, train_loader, criterion, optimizer, scheduler):\n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n  \n    for inputs, labels in train_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        optimizer.step()\n        scheduler.step() # потому что pytorch.__version__ > 1.1.0\n        \n        preds = torch.argmax(outputs, 1)\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_data += inputs.size(0)\n              \n    train_loss = running_loss / processed_data\n    train_acc = running_corrects.cpu().numpy() / processed_data\n    return train_loss, train_acc\n  \ndef eval_epoch(model, val_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    running_corrects = 0\n    processed_size = 0\n\n    for inputs, labels in val_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        with torch.set_grad_enabled(False):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            preds = torch.argmax(outputs, 1)\n\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_size += inputs.size(0)\n    val_loss = running_loss / processed_size\n    val_acc = running_corrects.double() / processed_size\n    return val_loss, val_acc\n  \ndef train(train_files, val_files, model, epochs, batch_size):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n\n        #***************************************************        \n        #opt = torch.optim.Adam(model.parameters())\n\n        #criterion = nn.CrossEntropyLoss()\n\n        #***************************************************\n        criterion = nn.CrossEntropyLoss()\n        #opt = torch.optim.SGD(model.parameters(), lr=1.)\n        opt = torch.optim.Adam(model.parameters())\n        step_size = 10*len(train_loader)\n        clr = cyclical_lr(step_size)\n        clr_scheduler = torch.optim.lr_scheduler.LambdaLR(opt, [clr])\n        #***************************************************\n\n        for epoch in range(epochs):\n            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt, clr_scheduler)\n            print(\"loss\", train_loss)\n            \n            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n            history.append((train_loss, train_acc, val_loss, val_acc))\n            \n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n            \n    return history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"RlsA478MoqDR"},"cell_type":"code","source":"def predict(model, test_loader):\n    with torch.no_grad():\n        logits = []\n    \n        for inputs in test_loader:\n            inputs = inputs.to(DEVICE)\n            model.eval()\n            outputs = model(inputs).cpu()\n            logits.append(outputs)\n            \n    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n    return probs","execution_count":null,"outputs":[]},{"metadata":{"id":"qTybfzXnoqDX"},"cell_type":"markdown","source":"# Actual training"},{"metadata":{"trusted":true,"id":"qnMkwvmHoqDY","outputId":"2eb3f36a-92b8-4cb0-bb12-46cc3bc800d4"},"cell_type":"code","source":"history = train(train_dataset, val_dataset, model=model, epochs=23, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"InnXOlKAoqDc"},"cell_type":"code","source":"loss, acc, val_loss, val_acc = zip(*history)","execution_count":null,"outputs":[]},{"metadata":{"id":"qDta74wMo--W"},"cell_type":"markdown","source":"Наконец, посмотрим на график:"},{"metadata":{"trusted":true,"id":"RaD9LKsWoqDg","outputId":"31947086-f433-486a-d62e-877532cd8edc"},"cell_type":"code","source":"loss, acc, val_loss, val_acc = zip(*history)\nfig = plt.figure(figsize=(20, 15))\n\nlosses = fig.add_subplot(2,2,3)\nlosses.plot(loss, label=\"train_loss\")\nlosses.plot(val_loss, label=\"val_loss\")\nlosses.legend(loc='best')\nlosses.grid(axis = 'y')\nlosses.set_xlabel(\"epochs\")\nlosses.set_ylabel(\"loss\")\n\naccs = fig.add_subplot(2,2,4)\naccs.plot(acc, label=\"train_acc\")\naccs.plot(val_acc, label=\"val_acc\")\naccs.legend(loc='best')\naccs.grid(axis = 'y')\naccs.set_xlabel(\"epochs\")\naccs.set_ylabel(\"accuracy\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"7ekUrCGpoqDp"},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true,"id":"16Vfk5cjoqDp"},"cell_type":"code","source":"label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n\ntest_dataset = SimpsonsDataset(test_files, mode=\"test\")\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\nprobs = predict(model, test_loader)\n\npreds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\ntest_filenames = [path.name for path in test_dataset.files]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"chn9F9ZDoqDr","outputId":"a29ca699-b164-4336-c4d4-8d3e3d01a635"},"cell_type":"code","source":"submit = pd.DataFrame({'Id': test_filenames, 'Expected': preds})\nsubmit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"jxZi8gEaoqDu"},"cell_type":"code","source":"submit.to_csv('model-resnet34_train100_batch64_epoch23.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"Copy of model-resnet18_train100_batch64_epoch30.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":4}